{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch tensors\n",
    "\n",
    "This notebook contains an introduction to PyTorch tensors.\n",
    "\n",
    "- [Creating tensors](#creating-tensors)\n",
    "  - [Scalars](#scalars)\n",
    "  - [Vectors](#vectors)\n",
    "  - [Matrices](#matrices)\n",
    "  - [Tensors (3D+)](#tensors-3d)\n",
    "  - [Creating random tensors](#creating-random-tensors)\n",
    "  - [Ones and Zeros](#ones-and-zeros)\n",
    "  - [Range of tensors and tensors-like](#range-of-tensors-and-tensors-like)\n",
    "- [Tensor's data types](#tensors-data-types)\n",
    "- [Requires Grad](#requires-grad)\n",
    "- [Reproducibility](#reproducibility)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting default device\n",
    "torch.set_default_device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7, device='mps:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scalar\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get tensor back as python int\n",
    "scalar.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get dimension of scalar\n",
    "scalar.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7], device='mps:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector\n",
    "vector = torch.tensor([7, 7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get dimension of vector\n",
    "vector.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shape of vector\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]], device='mps:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MATRIX\n",
    "MATRIX = torch.tensor(\n",
    "    [\n",
    "        [7, 8],\n",
    "        [9, 10],\n",
    "    ],\n",
    ")\n",
    "MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking dimensions of matrix\n",
    "MATRIX.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shape of matrix\n",
    "MATRIX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors (3D+)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TENSOR\n",
    "TENSOR = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [1, 2, 3],\n",
    "            [3, 6, 9],\n",
    "            [2, 4, 5],\n",
    "        ]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking dimensions of tensor\n",
    "TENSOR.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking shape of tensor\n",
    "TENSOR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating random tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For scalars and vectors, use lowercase letters, but for matrices or tensors, use uppercase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8523, 0.5161, 0.1860, 0.1054],\n",
       "         [0.8480, 0.4638, 0.8012, 0.4781],\n",
       "         [0.0817, 0.4037, 0.8348, 0.7221]]], device='mps:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random tensors\n",
    "random_tensor = torch.rand(1, 3, 4)\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random tensors are important, because the way many neural networks learn is that they start with tensors full of random numbers and then adjust those numbers to better represent the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking dimensions of tensor\n",
    "random_tensor.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([224, 224, 3]), 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating tensors of similar shape to an image\n",
    "random_image_tensor = torch.rand(size=(224, 224, 3))\n",
    "random_image_tensor.shape, random_image_tensor.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ones and Zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]], device='mps:0'),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 0.]], device='mps:0'))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors of ones and zeros\n",
    "ones = torch.ones(size=(3, 3))\n",
    "zeros = torch.zeros(size=(2, 3))\n",
    "ones, zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating tensors filled with zeros or ones (mostly the first) is useful for instances such as where you are creating a mask for an object. Masks are filters that determine what types of information is useful and which type should be ignored by the model. This is just one of the many applications of these tensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range of tensors and tensors-like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], device='mps:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor defined in range [a,b)\n",
    "one_to_ten = torch.arange(1, 11)\n",
    "one_to_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.arange` works similarly to the `range` built-in function in python, like the ability to set a step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='mps:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zeros tensor same shape as one_to_ten\n",
    "zeros_1_to_10 = torch.zeros_like(one_to_ten)\n",
    "zeros_1_to_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.(...)_like()` nomenclature denotes functions that allow the creation of tensors with some specific order (such as ones or zeros), but with the same shape as the tensor passed as argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if shapes are equal\n",
    "zeros_1_to_10.shape == one_to_ten.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor's data types\n",
    "\n",
    "**Note:** Tensor datatypes is one of the 3 big issues with you'll run into with PyTorch and deep learning:\n",
    "\n",
    "1. Tensors not the right datatype (this will happen in some functions, usually if the datatype is not `float32`)\n",
    "2. Tensors not the right shape\n",
    "3. Tensors not on the right device.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Float 32 tensor\n",
    "float_32_tensor = torch.tensor(\n",
    "    [3.0, 6.0, 9.0],\n",
    "    dtype=None,  # The datatype of the tensor (float32 by default)\n",
    "    device=None,  # The device where the tensor is located (cpu, gpu)\n",
    "    requires_grad=False,  # If True, records operations for automatic differentiation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision in computer science of a numerical quantity is a measure of the detail in which the quantity is expressed. Usually measured in bits but sometimes in decimal digits. It is related to precision in mathematics, which describes the number of digits that are used to express a value.\n",
    "\n",
    "- `float16` represents a 16-bit floating point (half precision)\n",
    "- `float32` represents a 32-bit floating point (single precision)\n",
    "- `float64` represents a 64-bit floating point (double precision)\n",
    "\n",
    "Half precision tensors are useful for situations where you sacrifice some detail in the numbers, and actual precision in exchange for numbers that are smaller.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking datatype\n",
    "float_32_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.], device='mps:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting tensor type\n",
    "float_16_tensor = float_32_tensor.type(torch.float16)\n",
    "float_16_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requires Grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requires_grad` parameter is kind of a deep explanation. Deep learning models use gradient descent to update weights and biases of a model during training. These weights and biases are nothing more than tensors, and the gradient \"tells them\" how they should adjust in order for the model to be better optimized.\n",
    "\n",
    "When this parameter is set to true, it allows for the computing of the gradient for the respective tensor. If, on the other hand, the parameter is false, it means you don't need gradients for that specific tensor (for inference/testing) or the tensor is input data, meaning it doesn't need to be learned.\n",
    "\n",
    "PyTorch records all operations on tensors with `require_grad=True`, building a computation graph. This graph enables automatic differentiation (backpropagation). It links all the operations involving the recorded tensor, allowing for the computation of its gradient.\n",
    "\n",
    "```python\n",
    "x = torch.tensor(3, requires_grad=True)\n",
    "\n",
    "y = x ** 2\n",
    "z = y + 4\n",
    "w = z * 2\n",
    "```\n",
    "\n",
    "The graph denominates x as a leaf node (can be checked with `x.is_leaf`), and the final output tensor (in our case w), is the root of the graph. The operation starts on the root and flows backward til the input tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "y = x**2\n",
    "z = y + 4\n",
    "w = z * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, False, False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if tensor is leaf\n",
    "x.is_leaf, y.is_leaf, z.is_leaf, w.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing backpropagation from w\n",
    "w.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12., device='mps:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking grad for x\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that a small variation in $x$ ($\\delta x$) would increase $w$ by 12 times that small increase. This is an indication that if $w$ increased, so would $x$. This means that, following the formula for updating the values in a neural network, using backpropagation, the operation would go something like:\n",
    "\n",
    "$$ x\\_{new} = x\\_{old} - \\eta \\cdot \\frac{\\partial w}{\\partial x}$$\n",
    "\n",
    "- For $\\eta = 0.1$ ($\\eta$ is the learning rate) :\n",
    "\n",
    "$$ x\\_{new} = 3 - 0.1 \\cdot 12$$\n",
    "\n",
    "$$ x\\_{new} = 3 - 1.2$$\n",
    "\n",
    "$$ x\\_{new} = 1.8$$\n",
    "\n",
    "Meaning the value of x should be updated to 1.8 in order for the function to be closer to being optimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and NumPy\n",
    "\n",
    "NumPy is a popular scientific Python numerical computing library. And because of this, PyTorch has functionality to interact with it. For instance, if you need a NumPy array to be converted into a tensor, you can use `torch.from_numpy_array(ndarray)`, or if you need the opposite, a tensor converted to a NumPy array, use `<tensor>.numpy()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]), tensor([1., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(1.0, 8.0)\n",
    "tensor = torch.from_numpy(array).type(torch.float32)\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default data type in NumPy is `float64` instead of PyTorch's `float32`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 3., 4., 5., 6., 7., 8.]), tensor([1., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding 1 to each value in the array\n",
    "array += 1\n",
    "\n",
    "# Checking array and tensor\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array and the tensor (that was created by converting from the array) don't share memory, meaning changes in one don't reflect on the other. This goes both ways, so tensors turned into arrays also don't share memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor to NumPy array\n",
    "tensor = torch.ones(7).cpu()  # Need to transfer from gpu (mps) to cpu because of numpy\n",
    "numpy_tensor = tensor.numpy()\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change in data type is bidirectional as well, PyTorch's default data type is `float32` and when a tensor is converted (that is in that format), the conversion is kept in NumPy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "\n",
    "The nature of neural networks is intertwined with the idea of \"randomness\", since the network for the most part starts with random numbers and tries to update these random numbers to make better representations of the data, this process happens over and over again. The thing is, in computing there isn't really any randomness, this is not even that desirable at times.\n",
    "\n",
    "This is a key concept in programming in general. There is no real way of computing a random number, instead we use employ techniques in order to make the data appear random, but there is always a trace, always a way of making this randomness not random whatsoever.\n",
    "\n",
    "This of course reflects on PyTorch, so we'll discuss how to do this. The why we don't want data to be actually random in this case is mostly for **_reproducibility_**, we need a way of reproducing the results in an experiment.\n",
    "\n",
    "This is done through setting the value of the \"**_random seed_**\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False, False],\n",
       "         [False, False, False, False, False]],\n",
       "\n",
       "        [[False, False, False, False, False],\n",
       "         [False, False, False, False, False]],\n",
       "\n",
       "        [[False, False, False, False, False],\n",
       "         [False, False, False, False, False]]], device='mps:0')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating two random tensors and checking whether they are equal\n",
    "random_tensor_a = torch.rand((3, 2, 5))\n",
    "random_tensor_b = torch.rand((3, 2, 5))\n",
    "\n",
    "# Checking if they are the same\n",
    "random_tensor_a == random_tensor_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating random seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True],\n",
       "         [True, True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True, True],\n",
       "         [True, True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True, True],\n",
       "         [True, True, True, True, True]]], device='mps:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating new tensors and comparing them\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor_c = torch.rand((3, 2, 5))\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor_d = torch.rand((3, 2, 5))\n",
    "\n",
    "# Checking if they are the same\n",
    "random_tensor_c == random_tensor_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, you need to set the manual seed for every process that generates random data. In our case we are doing two assignments, meaning two processes, therefore the seed is set manually twice.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
